{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPES_TO_INCLUDE = [\"class\", \"def\", \"identifier\", \"parameters\", \"argument_list\", \"->\", \":\", \"type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List\n",
    "\n",
    "\n",
    "def include_parent(\n",
    "    node: Any,\n",
    "    text: str,\n",
    "    include_parent_attributes: bool,\n",
    "    include_parent_init: bool,\n",
    "    depth: int,\n",
    "    buffer: str,\n",
    ") -> str:\n",
    "    \"\"\"Recursively include the parent of a node in the buffer. Depth 1 includes the same level nodes, depth 2 includes the same level and the parent's level, etc.\"\"\"\n",
    "    if node.type == \"block\":\n",
    "        parent = node.parent\n",
    "    else:\n",
    "        parent = node.parent.parent\n",
    "        if parent is None or parent.type == \"module\":\n",
    "            # pass for root or module\n",
    "            return buffer\n",
    "        if parent.type == \"block\":\n",
    "            # depending which level the node starts, a single level parent could be a block. We need a class of function definition which is one level up.\n",
    "            parent = parent.parent\n",
    "\n",
    "    parent_str = \"\"\n",
    "    byte_ranges = []\n",
    "    for child in parent.children:\n",
    "        if child.type in TYPES_TO_INCLUDE:\n",
    "            byte_ranges.append(child.byte_range)\n",
    "\n",
    "    # merge byte ranges\n",
    "    new_byte_ranges = []\n",
    "    merged_byte_range = ()\n",
    "    (1,3), (4,6), (7,9)\n",
    "    for i in range(len(byte_ranges) - 1):\n",
    "        if byte_ranges[i][1] == byte_ranges[i + 1][0] - 1:\n",
    "            if merged_byte_range == ():\n",
    "                merged_byte_range = (byte_ranges[i][0], byte_ranges[i + 1][1])\n",
    "            else:\n",
    "                merged_byte_range = (merged_byte_range[0], byte_ranges[i + 1][1])\n",
    "        else:\n",
    "            if merged_byte_range == ():\n",
    "                new_byte_ranges.append(byte_ranges[i])\n",
    "            else:\n",
    "                new_byte_ranges.append(merged_byte_range)\n",
    "                merged_byte_range = ()\n",
    "            if i == len(byte_ranges) - 2:\n",
    "                new_byte_ranges.append(byte_ranges[i + 1])\n",
    "\n",
    "    for byte_range in new_byte_ranges:\n",
    "        reach_the_begin = False\n",
    "        left_end_byte_of_chunk = byte_range[0]\n",
    "        while not reach_the_begin:\n",
    "            if text[left_end_byte_of_chunk - 1] != \" \":\n",
    "                reach_the_begin = True\n",
    "            else:\n",
    "                left_end_byte_of_chunk -= 1\n",
    "        parent_str += text[left_end_byte_of_chunk : byte_range[1] + 1]\n",
    "\n",
    "    # Add extra information about the parent node\n",
    "    if include_parent_attributes:\n",
    "        pass\n",
    "\n",
    "    if include_parent_init:\n",
    "        pass\n",
    "\n",
    "    if depth == 1:  # last level\n",
    "        return parent_str + \"\\n\" + buffer\n",
    "    else:\n",
    "        return include_parent(\n",
    "            parent,\n",
    "            text,\n",
    "            include_parent_attributes,\n",
    "            include_parent_init,\n",
    "            depth - 1,\n",
    "            parent_str + \"\\n\" + buffer,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_node(\n",
    "    node: Any,\n",
    "    text: str,\n",
    "    last_end: int = 0,\n",
    "    max_chars: int = 1500,\n",
    "    include_related_imports: bool = True,\n",
    "    include_parent_depth: int = 1,\n",
    "    include_parent_attributes: bool = False,\n",
    "    summarize_parent_attributes: bool = False,\n",
    "    include_parent_init: bool = False,\n",
    "    summarize_parent_init: bool = False,\n",
    ") -> List[str]:\n",
    "    new_chunks = []\n",
    "    imports = []\n",
    "    current_chunk = \"\"\n",
    "    is_first_node_for_chunk = True\n",
    "    for child in node.children:\n",
    "        if child.end_byte - child.start_byte > max_chars:\n",
    "            # Child is too big, recursively chunk the child\n",
    "            if len(current_chunk) > 0:\n",
    "                new_chunks.append(current_chunk)\n",
    "            current_chunk = \"\"\n",
    "            new_chunks.extend(\n",
    "                chunk_node(\n",
    "                    child,\n",
    "                    text,\n",
    "                    last_end,\n",
    "                    max_chars,\n",
    "                    include_related_imports,\n",
    "                    include_parent_depth,\n",
    "                    include_parent_attributes,\n",
    "                    summarize_parent_attributes,\n",
    "                    include_parent_init,\n",
    "                    summarize_parent_init\n",
    "                )\n",
    "            )\n",
    "            is_first_node_for_chunk = True\n",
    "        else:\n",
    "            # add related parts only for the first node of a chunk\n",
    "            if is_first_node_for_chunk and include_parent_depth > 0 and len(current_chunk.strip()) == 0:\n",
    "                parent_information = include_parent(\n",
    "                    child,\n",
    "                    text,\n",
    "                    include_parent_attributes,\n",
    "                    include_parent_init,\n",
    "                    include_parent_depth,\n",
    "                    \"\",\n",
    "                )\n",
    "                current_chunk = parent_information + text[last_end : child.end_byte]\n",
    "            else:\n",
    "                if len(current_chunk) + child.end_byte - last_end < max_chars:\n",
    "                    current_chunk += text[last_end : child.end_byte]\n",
    "                else:\n",
    "                    new_chunks.append(current_chunk)\n",
    "                    if include_parent_depth > 0:\n",
    "                        parent_information = include_parent(\n",
    "                            child,\n",
    "                            text,\n",
    "                            include_parent_attributes,\n",
    "                            include_parent_init,\n",
    "                            include_parent_depth,\n",
    "                            \"\",\n",
    "                        )\n",
    "                        current_chunk = (\n",
    "                            parent_information\n",
    "                            + text[last_end : child.end_byte]\n",
    "                        )\n",
    "                    else:\n",
    "                        current_chunk = text[last_end : child.end_byte]\n",
    "                    \n",
    "            is_first_node_for_chunk = False\n",
    "\n",
    "        last_end = child.end_byte\n",
    "\n",
    "    if len(current_chunk) > 0:\n",
    "        new_chunks.append(current_chunk)\n",
    "\n",
    "    return new_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tree_sitter_languages\n",
    "parser = tree_sitter_languages.get_parser(\"python\")\n",
    "\n",
    "with open(\n",
    "    \"/Users/minkijung/Documents/2PetProjects/AI_README_Generator/testing/test2.py\", \"r\"\n",
    ") as file:\n",
    "    text = file.read()\n",
    "tree = parser.parse(bytes(text, \"utf-8\"))\n",
    "root = tree.root_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = chunk_node(root, text, 0, 1500, 2, True, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from __future__ import annotations\n",
      "\n",
      "from typing import TYPE_CHECKING, Any, Dict, List, Optional, Sequence, Union, cast\n",
      "\n",
      "from langchain_core.load.serializable import Serializable\n",
      "from langchain_core.pydantic_v1 import Extra, Field\n",
      "from langchain_core.utils import get_bolded_text\n",
      "from langchain_core.utils._merge import merge_dicts, merge_lists\n",
      "from langchain_core.utils.interactive_env import is_interactive_env\n",
      "\n",
      "if TYPE_CHECKING:\n",
      "    from langchain_core.prompts.chat import ChatPromptTemplate\n",
      "---------------\n",
      "\n",
      "\n",
      "\n",
      "class BaseMessage(Serializable):\n",
      "---------------\n",
      "class BaseMessage((Serializable)::\n",
      "\n",
      "\n",
      "    \"\"\"Base abstract message class.\n",
      "\n",
      "    Messages are the inputs and outputs of ChatModels.\n",
      "    \"\"\"\n",
      "\n",
      "    content: Union[str, List[Union[str, Dict]]]\n",
      "    \"\"\"The string contents of the message.\"\"\"\n",
      "\n",
      "    additional_kwargs: dict = Field(default_factory=dict)\n",
      "    \"\"\"Reserved for additional payload data associated with the message.\n",
      "    \n",
      "    For example, for a message from an AI, this could include tool calls as\n",
      "    encoded by the model provider.\n",
      "    \"\"\"\n",
      "\n",
      "    response_metadata: dict = Field(default_factory=dict)\n",
      "    \"\"\"Response metadata. For example: response headers, logprobs, token counts.\"\"\"\n",
      "\n",
      "    type: str\n",
      "    \"\"\"The type of the message. Must be a string that is unique to the message type.\n",
      "    \n",
      "    The purpose of this field is to allow for easy identification of the message type\n",
      "    when deserializing messages.\n",
      "    \"\"\"\n",
      "\n",
      "    name: Optional[str] = None\n",
      "    \"\"\"An optional name for the message. \n",
      "    \n",
      "    This can be used to provide a human-readable name for the message.\n",
      "    \n",
      "    Usage of this field is optional, and whether it's used or not is up to the\n",
      "    model implementation.\n",
      "    \"\"\"\n",
      "\n",
      "    id: Optional[str] = None\n",
      "    \"\"\"An optional unique identifier for the message. This should ideally be\n",
      "    provided by the provider/model which created the message.\"\"\"\n",
      "\n",
      "    class Config:\n",
      "        extra = Extra.allow\n",
      "---------------\n",
      "class BaseMessage((Serializable)::\n",
      "\n",
      "\n",
      "\n",
      "    def __init__(\n",
      "        self, content: Union[str, List[Union[str, Dict]]], **kwargs: Any\n",
      "    ) -> None:\n",
      "        \"\"\"Pass in content as positional arg.\n",
      "\n",
      "        Args:\n",
      "            content: The string contents of the message.\n",
      "            **kwargs: Additional fields to pass to the\n",
      "        \"\"\"\n",
      "        super().__init__(content=content, **kwargs)\n",
      "\n",
      "    @classmethod\n",
      "    def is_lc_serializable(cls) -> bool:\n",
      "        \"\"\"Return whether this class is serializable. This is used to determine\n",
      "        whether the class should be included in the langchain schema.\n",
      "\n",
      "        Returns:\n",
      "            True if the class is serializable, False otherwise.\n",
      "        \"\"\"\n",
      "        return True\n",
      "\n",
      "    @classmethod\n",
      "    def get_lc_namespace(cls) -> List[str]:\n",
      "        \"\"\"Get the namespace of the langchain object.\n",
      "        Default is [\"langchain\", \"schema\", \"messages\"].\n",
      "        \"\"\"\n",
      "        return [\"langchain\", \"schema\", \"messages\"]\n",
      "\n",
      "    def __add__(self, other: Any) -> ChatPromptTemplate:\n",
      "        \"\"\"Concatenate this message with another message.\"\"\"\n",
      "        from langchain_core.prompts.chat import ChatPromptTemplate\n",
      "\n",
      "        prompt = ChatPromptTemplate(messages=[self])  # type: ignore[call-arg]\n",
      "        return prompt + other\n",
      "---------------\n",
      "class BaseMessage((Serializable)::\n",
      "\n",
      "\n",
      "\n",
      "    def pretty_repr(self, html: bool = False) -> str:\n",
      "        \"\"\"Get a pretty representation of the message.\n",
      "\n",
      "        Args:\n",
      "            html: Whether to format the message as HTML. If True, the message will be\n",
      "                formatted with HTML tags. Default is False.\n",
      "\n",
      "        Returns:\n",
      "            A pretty representation of the message.\n",
      "        \"\"\"\n",
      "        title = get_msg_title_repr(self.type.title() + \" Message\", bold=html)\n",
      "        # TODO: handle non-string content.\n",
      "        if self.name is not None:\n",
      "            title += f\"\\nName: {self.name}\"\n",
      "        return f\"{title}\\n\\n{self.content}\"\n",
      "\n",
      "    def pretty_print(self) -> None:\n",
      "        print(self.pretty_repr(html=is_interactive_env()))  # noqa: T201\n",
      "---------------\n",
      "\n",
      "\n",
      "\n",
      "def merge_content(\n",
      "    first_content: Union[str, List[Union[str, Dict]]],\n",
      "    *contents: Union[str, List[Union[str, Dict]]],\n",
      ") -> Union[str, List[Union[str, Dict]]]:\n",
      "---------------\n",
      "def merge_content((\n",
      "    first_content: Union[str, List[Union[str, Dict]]],\n",
      "    *contents: Union[str, List[Union[str, Dict]]],\n",
      ") -> Union[str, List[Union[str, Dict]]]::\n",
      "\n",
      "\n",
      "    \"\"\"Merge two message contents.\n",
      "\n",
      "    Args:\n",
      "        first_content: The first content. Can be a string or a list.\n",
      "        second_content: The second content. Can be a string or a list.\n",
      "\n",
      "    Returns:\n",
      "        The merged content.\n",
      "    \"\"\"\n",
      "    merged = first_content\n",
      "    for content in contents:\n",
      "        # If current is a string\n",
      "        if isinstance(merged, str):\n",
      "            # If the next chunk is also a string, then merge them naively\n",
      "            if isinstance(content, str):\n",
      "                merged = cast(str, merged) + content\n",
      "            # If the next chunk is a list, add the current to the start of the list\n",
      "            else:\n",
      "                merged = [merged] + content  # type: ignore\n",
      "        elif isinstance(content, list):\n",
      "            # If both are lists\n",
      "            merged = merge_lists(cast(List, merged), content)  # type: ignore\n",
      "        # If the first content is a list, and the second content is a string\n",
      "        else:\n",
      "            # If the last element of the first content is a string\n",
      "            # Add the second content to the last element\n",
      "            if merged and isinstance(merged[-1], str):\n",
      "                merged[-1] += content\n",
      "            # If second content is an empty string, treat as a no-op\n",
      "            elif content == \"\":\n",
      "                pass\n",
      "            else:\n",
      "                # Otherwise, add the second content as a new element of the list\n",
      "                merged.append(content)\n",
      "    return merged\n",
      "---------------\n",
      "\n",
      "\n",
      "\n",
      "class BaseMessageChunk(BaseMessage):\n",
      "---------------\n",
      "class BaseMessageChunk((BaseMessage)::\n",
      "\n",
      "\n",
      "    \"\"\"Message chunk, which can be concatenated with other Message chunks.\"\"\"\n",
      "\n",
      "    @classmethod\n",
      "    def get_lc_namespace(cls) -> List[str]:\n",
      "        \"\"\"Get the namespace of the langchain object.\n",
      "        Default is [\"langchain\", \"schema\", \"messages\"].\n",
      "        \"\"\"\n",
      "        return [\"langchain\", \"schema\", \"messages\"]\n",
      "---------------\n",
      "class BaseMessageChunk((BaseMessage)::\n",
      "\n",
      "\n",
      "\n",
      "    def __add__(self, other: Any) -> BaseMessageChunk:  # type: ignore\n",
      "---------------\n",
      "    def __add__((self, other: Any) -> BaseMessageChunk:: \n",
      "\n",
      "        \"\"\"Message chunks support concatenation with other message chunks.\n",
      "\n",
      "        This functionality is useful to combine message chunks yielded from\n",
      "        a streaming model into a complete message.\n",
      "\n",
      "        Args:\n",
      "            other: Another message chunk to concatenate with this one.\n",
      "\n",
      "        Returns:\n",
      "            A new message chunk that is the concatenation of this message chunk\n",
      "            and the other message chunk.\n",
      "\n",
      "        Raises:\n",
      "            TypeError: If the other object is not a message chunk.\n",
      "\n",
      "        For example,\n",
      "\n",
      "        `AIMessageChunk(content=\"Hello\") + AIMessageChunk(content=\" World\")`\n",
      "\n",
      "        will give `AIMessageChunk(content=\"Hello World\")`\n",
      "        \"\"\"\n",
      "---------------\n",
      "    def __add__((self, other: Any) -> BaseMessageChunk:: \n",
      "\n",
      "        if isinstance(other, BaseMessageChunk):\n",
      "            # If both are (subclasses of) BaseMessageChunk,\n",
      "            # concat into a single BaseMessageChunk\n",
      "\n",
      "            return self.__class__(  # type: ignore[call-arg]\n",
      "                id=self.id,\n",
      "                content=merge_content(self.content, other.content),\n",
      "                additional_kwargs=merge_dicts(\n",
      "                    self.additional_kwargs, other.additional_kwargs\n",
      "                ),\n",
      "                response_metadata=merge_dicts(\n",
      "                    self.response_metadata, other.response_metadata\n",
      "                ),\n",
      "            )\n",
      "        elif isinstance(other, list) and all(\n",
      "            isinstance(o, BaseMessageChunk) for o in other\n",
      "        ):\n",
      "            content = merge_content(self.content, *(o.content for o in other))\n",
      "            additional_kwargs = merge_dicts(\n",
      "                self.additional_kwargs, *(o.additional_kwargs for o in other)\n",
      "            )\n",
      "            response_metadata = merge_dicts(\n",
      "                self.response_metadata, *(o.response_metadata for o in other)\n",
      "            )\n",
      "            return self.__class__(  # type: ignore[call-arg]\n",
      "                id=self.id,\n",
      "                content=content,\n",
      "                additional_kwargs=additional_kwargs,\n",
      "                response_metadata=response_metadata,\n",
      "            )\n",
      "---------------\n",
      "    def __add__((self, other: Any) -> BaseMessageChunk:: \n",
      "\n",
      "        else:\n",
      "            raise TypeError(\n",
      "                'unsupported operand type(s) for +: \"'\n",
      "                f\"{self.__class__.__name__}\"\n",
      "                f'\" and \"{other.__class__.__name__}\"'\n",
      "            )\n",
      "---------------\n",
      "\n",
      "\n",
      "\n",
      "def message_to_dict(message: BaseMessage) -> dict:\n",
      "    \"\"\"Convert a Message to a dictionary.\n",
      "\n",
      "    Args:\n",
      "        message: Message to convert.\n",
      "\n",
      "    Returns:\n",
      "        Message as a dict. The dict will have a \"type\" key with the message type\n",
      "        and a \"data\" key with the message data as a dict.\n",
      "    \"\"\"\n",
      "    return {\"type\": message.type, \"data\": message.dict()}\n",
      "\n",
      "\n",
      "def messages_to_dict(messages: Sequence[BaseMessage]) -> List[dict]:\n",
      "    \"\"\"Convert a sequence of Messages to a list of dictionaries.\n",
      "\n",
      "    Args:\n",
      "        messages: Sequence of messages (as BaseMessages) to convert.\n",
      "\n",
      "    Returns:\n",
      "        List of messages as dicts.\n",
      "    \"\"\"\n",
      "    return [message_to_dict(m) for m in messages]\n",
      "\n",
      "\n",
      "def get_msg_title_repr(title: str, *, bold: bool = False) -> str:\n",
      "    \"\"\"Get a title representation for a message.\n",
      "\n",
      "    Args:\n",
      "        title: The title.\n",
      "        bold: Whether to bold the title. Default is False.\n",
      "\n",
      "    Returns:\n",
      "        The title representation.\n",
      "    \"\"\"\n",
      "    padded = \" \" + title + \" \"\n",
      "    sep_len = (80 - len(padded)) // 2\n",
      "    sep = \"=\" * sep_len\n",
      "    second_sep = sep + \"=\" if len(padded) % 2 else sep\n",
      "    if bold:\n",
      "        padded = get_bolded_text(padded)\n",
      "    return f\"{sep}{padded}{second_sep}\"\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "for i,result in enumerate(chunks):\n",
    "    print(result)\n",
    "    print(\"---------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language:  python\n",
      "\"\"\"Code splitter.\"\"\"\n",
      "\n",
      "import os\n",
      "from typing import Any, Callable, List, Optional\n",
      "\n",
      "from llama_index.core.bridge.pydantic import Field, PrivateAttr\n",
      "from llama_index.core.callbacks.base import CallbackManager\n",
      "from llama_index.core.callbacks.schema import CBEventType, EventPayload\n",
      "from llama_index.core.node_parser.interface import TextSplitter\n",
      "from llama_index.core.node_parser.node_utils import default_id_func\n",
      "from llama_index.core.schema import Document\n",
      "\n",
      "DEFAULT_CHUNK_LINES = 40\n",
      "DEFAULT_LINES_OVERLAP = 15\n",
      "DEFAULT_MAX_CHARS = 1500\n",
      "---------------\n",
      "class CodeSplitter(TextSplitter):\n",
      "---------------\n",
      "\"\"\"Split code using a AST parser.\n",
      "\n",
      "    Thank you to Kevin Lu / SweepAI for suggesting this elegant code splitting solution.\n",
      "    https://docs.sweep.dev/blogs/chunking-2m-files\n",
      "    \"\"\"\n",
      "\n",
      "    language: str = Field(\n",
      "        description=\"The programming language of the code being split.\"\n",
      "    )\n",
      "    chunk_lines: int = Field(\n",
      "        default=DEFAULT_CHUNK_LINES,\n",
      "        description=\"The number of lines to include in each chunk.\",\n",
      "        gt=0,\n",
      "    )\n",
      "    chunk_lines_overlap: int = Field(\n",
      "        default=DEFAULT_LINES_OVERLAP,\n",
      "        description=\"How many lines of code each chunk overlaps with.\",\n",
      "        gt=0,\n",
      "    )\n",
      "    max_chars: int = Field(\n",
      "        default=DEFAULT_MAX_CHARS,\n",
      "        description=\"Maximum number of characters per chunk.\",\n",
      "        gt=0,\n",
      "    )\n",
      "    _parser: Any = PrivateAttr()\n",
      "---------------\n",
      "def __init__(\n",
      "        self,\n",
      "        language: str,\n",
      "        chunk_lines: int = DEFAULT_CHUNK_LINES,\n",
      "        chunk_lines_overlap: int = DEFAULT_LINES_OVERLAP,\n",
      "        max_chars: int = DEFAULT_MAX_CHARS,\n",
      "        parser: Any = None,\n",
      "        callback_manager: Optional[CallbackManager] = None,\n",
      "        include_metadata: bool = True,\n",
      "        include_prev_next_rel: bool = True,\n",
      "        id_func: Optional[Callable[[int, Document], str]] = None,\n",
      "    ) -> None:\n",
      "---------------\n",
      "\"\"\"Initialize a CodeSplitter.\"\"\"\n",
      "        from tree_sitter import Parser  # pants: no-infer-dep\n",
      "\n",
      "        if parser is None:\n",
      "            try:\n",
      "                import tree_sitter_languages  # pants: no-infer-dep\n",
      "\n",
      "                print(\"language: \", language)\n",
      "                parser = tree_sitter_languages.get_parser(language)\n",
      "            except ImportError:\n",
      "                raise ImportError(\n",
      "                    \"Please install tree_sitter_languages to use CodeSplitter.\"\n",
      "                    \"Or pass in a parser object.\"\n",
      "                )\n",
      "            except Exception:\n",
      "                print(\n",
      "                    f\"Could not get parser for language {language}. Check \"\n",
      "                    \"https://github.com/grantjenks/py-tree-sitter-languages#license \"\n",
      "                    \"for a list of valid languages.\"\n",
      "                )\n",
      "                raise\n",
      "        if not isinstance(parser, Parser):\n",
      "            raise ValueError(\"Parser must be a tree-sitter Parser object.\")\n",
      "\n",
      "        self._parser = parser\n",
      "\n",
      "        callback_manager = callback_manager or CallbackManager([])\n",
      "        id_func = id_func or default_id_func\n",
      "\n",
      "        super().__init__(\n",
      "            language=language,\n",
      "            chunk_lines=chunk_lines,\n",
      "            chunk_lines_overlap=chunk_lines_overlap,\n",
      "            max_chars=max_chars,\n",
      "            callback_manager=callback_manager,\n",
      "            include_metadata=include_metadata,\n",
      "            include_prev_next_rel=include_prev_next_rel,\n",
      "            id_func=id_func,\n",
      "        )\n",
      "---------------\n",
      "@classmethod\n",
      "    def from_defaults(\n",
      "        cls,\n",
      "        language: str,\n",
      "        chunk_lines: int = DEFAULT_CHUNK_LINES,\n",
      "        chunk_lines_overlap: int = DEFAULT_LINES_OVERLAP,\n",
      "        max_chars: int = DEFAULT_MAX_CHARS,\n",
      "        callback_manager: Optional[CallbackManager] = None,\n",
      "        parser: Any = None,\n",
      "    ) -> \"CodeSplitter\":\n",
      "        \"\"\"Create a CodeSplitter with default values.\"\"\"\n",
      "        return cls(\n",
      "            language=language,\n",
      "            chunk_lines=chunk_lines,\n",
      "            chunk_lines_overlap=chunk_lines_overlap,\n",
      "            max_chars=max_chars,\n",
      "            parser=parser,\n",
      "        )\n",
      "\n",
      "    @classmethod\n",
      "    def class_name(cls) -> str:\n",
      "        return \"CodeSplitter\"\n",
      "---------------\n",
      "def _chunk_node(self, node: Any, text: str, last_end: int = 0) -> List[str]:\n",
      "        new_chunks = []\n",
      "        current_chunk = \"\"\n",
      "        for child in node.children:\n",
      "            if child.end_byte - child.start_byte > self.max_chars:\n",
      "                # Child is too big, recursively chunk the child\n",
      "                if len(current_chunk) > 0:\n",
      "                    new_chunks.append(current_chunk)\n",
      "                current_chunk = \"\"\n",
      "                new_chunks.extend(self._chunk_node(child, text, last_end))\n",
      "            elif (\n",
      "                len(current_chunk) + child.end_byte - child.start_byte > self.max_chars\n",
      "            ):\n",
      "                # Child would make the current chunk too big, so start a new chunk\n",
      "                new_chunks.append(current_chunk)\n",
      "                current_chunk = text[last_end : child.end_byte]\n",
      "            else:\n",
      "                current_chunk += text[last_end : child.end_byte]\n",
      "            last_end = child.end_byte\n",
      "        if len(current_chunk) > 0:\n",
      "            new_chunks.append(current_chunk)\n",
      "        return new_chunks\n",
      "---------------\n",
      "def split_text(self, text: str) -> List[str]:\n",
      "        \"\"\"Split incoming code and return chunks using the AST.\"\"\"\n",
      "        with self.callback_manager.event(\n",
      "            CBEventType.CHUNKING, payload={EventPayload.CHUNKS: [text]}\n",
      "        ) as event:\n",
      "            tree = self._parser.parse(bytes(text, \"utf-8\"))\n",
      "\n",
      "            if (\n",
      "                not tree.root_node.children\n",
      "                or tree.root_node.children[0].type != \"ERROR\"\n",
      "            ):\n",
      "                chunks = [\n",
      "                    chunk.strip() for chunk in self._chunk_node(tree.root_node, text)\n",
      "                ]\n",
      "                event.on_end(\n",
      "                    payload={EventPayload.CHUNKS: chunks},\n",
      "                )\n",
      "\n",
      "                return chunks\n",
      "            else:\n",
      "                raise ValueError(f\"Could not parse code with language {self.language}.\")\n",
      "\n",
      "        # TODO: set up auto-language detection using something like https://github.com/yoeo/guesslang.\n",
      "---------------\n",
      "from __future__ import annotations\n",
      "\n",
      "from typing import TYPE_CHECKING, Any, Dict, List, Optional, Sequence, Union, cast\n",
      "\n",
      "from langchain_core.load.serializable import Serializable\n",
      "from langchain_core.pydantic_v1 import Extra, Field\n",
      "from langchain_core.utils import get_bolded_text\n",
      "from langchain_core.utils._merge import merge_dicts, merge_lists\n",
      "from langchain_core.utils.interactive_env import is_interactive_env\n",
      "\n",
      "if TYPE_CHECKING:\n",
      "    from langchain_core.prompts.chat import ChatPromptTemplate\n",
      "---------------\n",
      "class BaseMessage(Serializable):\n",
      "---------------\n",
      "\"\"\"Base abstract message class.\n",
      "\n",
      "    Messages are the inputs and outputs of ChatModels.\n",
      "    \"\"\"\n",
      "\n",
      "    content: Union[str, List[Union[str, Dict]]]\n",
      "    \"\"\"The string contents of the message.\"\"\"\n",
      "\n",
      "    additional_kwargs: dict = Field(default_factory=dict)\n",
      "    \"\"\"Reserved for additional payload data associated with the message.\n",
      "    \n",
      "    For example, for a message from an AI, this could include tool calls as\n",
      "    encoded by the model provider.\n",
      "    \"\"\"\n",
      "\n",
      "    response_metadata: dict = Field(default_factory=dict)\n",
      "    \"\"\"Response metadata. For example: response headers, logprobs, token counts.\"\"\"\n",
      "\n",
      "    type: str\n",
      "    \"\"\"The type of the message. Must be a string that is unique to the message type.\n",
      "    \n",
      "    The purpose of this field is to allow for easy identification of the message type\n",
      "    when deserializing messages.\n",
      "    \"\"\"\n",
      "\n",
      "    name: Optional[str] = None\n",
      "    \"\"\"An optional name for the message. \n",
      "    \n",
      "    This can be used to provide a human-readable name for the message.\n",
      "    \n",
      "    Usage of this field is optional, and whether it's used or not is up to the\n",
      "    model implementation.\n",
      "    \"\"\"\n",
      "\n",
      "    id: Optional[str] = None\n",
      "    \"\"\"An optional unique identifier for the message. This should ideally be\n",
      "    provided by the provider/model which created the message.\"\"\"\n",
      "\n",
      "    class Config:\n",
      "        extra = Extra.allow\n",
      "---------------\n",
      "def __init__(\n",
      "        self, content: Union[str, List[Union[str, Dict]]], **kwargs: Any\n",
      "    ) -> None:\n",
      "        \"\"\"Pass in content as positional arg.\n",
      "\n",
      "        Args:\n",
      "            content: The string contents of the message.\n",
      "            **kwargs: Additional fields to pass to the\n",
      "        \"\"\"\n",
      "        super().__init__(content=content, **kwargs)\n",
      "\n",
      "    @classmethod\n",
      "    def is_lc_serializable(cls) -> bool:\n",
      "        \"\"\"Return whether this class is serializable. This is used to determine\n",
      "        whether the class should be included in the langchain schema.\n",
      "\n",
      "        Returns:\n",
      "            True if the class is serializable, False otherwise.\n",
      "        \"\"\"\n",
      "        return True\n",
      "\n",
      "    @classmethod\n",
      "    def get_lc_namespace(cls) -> List[str]:\n",
      "        \"\"\"Get the namespace of the langchain object.\n",
      "        Default is [\"langchain\", \"schema\", \"messages\"].\n",
      "        \"\"\"\n",
      "        return [\"langchain\", \"schema\", \"messages\"]\n",
      "\n",
      "    def __add__(self, other: Any) -> ChatPromptTemplate:\n",
      "        \"\"\"Concatenate this message with another message.\"\"\"\n",
      "        from langchain_core.prompts.chat import ChatPromptTemplate\n",
      "\n",
      "        prompt = ChatPromptTemplate(messages=[self])  # type: ignore[call-arg]\n",
      "        return prompt + other\n",
      "---------------\n",
      "def pretty_repr(self, html: bool = False) -> str:\n",
      "        \"\"\"Get a pretty representation of the message.\n",
      "\n",
      "        Args:\n",
      "            html: Whether to format the message as HTML. If True, the message will be\n",
      "                formatted with HTML tags. Default is False.\n",
      "\n",
      "        Returns:\n",
      "            A pretty representation of the message.\n",
      "        \"\"\"\n",
      "        title = get_msg_title_repr(self.type.title() + \" Message\", bold=html)\n",
      "        # TODO: handle non-string content.\n",
      "        if self.name is not None:\n",
      "            title += f\"\\nName: {self.name}\"\n",
      "        return f\"{title}\\n\\n{self.content}\"\n",
      "\n",
      "    def pretty_print(self) -> None:\n",
      "        print(self.pretty_repr(html=is_interactive_env()))  # noqa: T201\n",
      "---------------\n",
      "def merge_content(\n",
      "    first_content: Union[str, List[Union[str, Dict]]],\n",
      "    *contents: Union[str, List[Union[str, Dict]]],\n",
      ") -> Union[str, List[Union[str, Dict]]]:\n",
      "---------------\n",
      "\"\"\"Merge two message contents.\n",
      "\n",
      "    Args:\n",
      "        first_content: The first content. Can be a string or a list.\n",
      "        second_content: The second content. Can be a string or a list.\n",
      "\n",
      "    Returns:\n",
      "        The merged content.\n",
      "    \"\"\"\n",
      "    merged = first_content\n",
      "    for content in contents:\n",
      "        # If current is a string\n",
      "        if isinstance(merged, str):\n",
      "            # If the next chunk is also a string, then merge them naively\n",
      "            if isinstance(content, str):\n",
      "                merged = cast(str, merged) + content\n",
      "            # If the next chunk is a list, add the current to the start of the list\n",
      "            else:\n",
      "                merged = [merged] + content  # type: ignore\n",
      "        elif isinstance(content, list):\n",
      "            # If both are lists\n",
      "            merged = merge_lists(cast(List, merged), content)  # type: ignore\n",
      "        # If the first content is a list, and the second content is a string\n",
      "        else:\n",
      "            # If the last element of the first content is a string\n",
      "            # Add the second content to the last element\n",
      "            if merged and isinstance(merged[-1], str):\n",
      "                merged[-1] += content\n",
      "            # If second content is an empty string, treat as a no-op\n",
      "            elif content == \"\":\n",
      "                pass\n",
      "            else:\n",
      "                # Otherwise, add the second content as a new element of the list\n",
      "                merged.append(content)\n",
      "    return merged\n",
      "---------------\n",
      "class BaseMessageChunk(BaseMessage):\n",
      "---------------\n",
      "\"\"\"Message chunk, which can be concatenated with other Message chunks.\"\"\"\n",
      "\n",
      "    @classmethod\n",
      "    def get_lc_namespace(cls) -> List[str]:\n",
      "        \"\"\"Get the namespace of the langchain object.\n",
      "        Default is [\"langchain\", \"schema\", \"messages\"].\n",
      "        \"\"\"\n",
      "        return [\"langchain\", \"schema\", \"messages\"]\n",
      "---------------\n",
      "def __add__(self, other: Any) -> BaseMessageChunk:  # type: ignore\n",
      "---------------\n",
      "\"\"\"Message chunks support concatenation with other message chunks.\n",
      "\n",
      "        This functionality is useful to combine message chunks yielded from\n",
      "        a streaming model into a complete message.\n",
      "\n",
      "        Args:\n",
      "            other: Another message chunk to concatenate with this one.\n",
      "\n",
      "        Returns:\n",
      "            A new message chunk that is the concatenation of this message chunk\n",
      "            and the other message chunk.\n",
      "\n",
      "        Raises:\n",
      "            TypeError: If the other object is not a message chunk.\n",
      "\n",
      "        For example,\n",
      "\n",
      "        `AIMessageChunk(content=\"Hello\") + AIMessageChunk(content=\" World\")`\n",
      "\n",
      "        will give `AIMessageChunk(content=\"Hello World\")`\n",
      "        \"\"\"\n",
      "---------------\n",
      "if isinstance(other, BaseMessageChunk):\n",
      "            # If both are (subclasses of) BaseMessageChunk,\n",
      "            # concat into a single BaseMessageChunk\n",
      "\n",
      "            return self.__class__(  # type: ignore[call-arg]\n",
      "                id=self.id,\n",
      "                content=merge_content(self.content, other.content),\n",
      "                additional_kwargs=merge_dicts(\n",
      "                    self.additional_kwargs, other.additional_kwargs\n",
      "                ),\n",
      "                response_metadata=merge_dicts(\n",
      "                    self.response_metadata, other.response_metadata\n",
      "                ),\n",
      "            )\n",
      "        elif isinstance(other, list) and all(\n",
      "            isinstance(o, BaseMessageChunk) for o in other\n",
      "        ):\n",
      "            content = merge_content(self.content, *(o.content for o in other))\n",
      "            additional_kwargs = merge_dicts(\n",
      "                self.additional_kwargs, *(o.additional_kwargs for o in other)\n",
      "            )\n",
      "            response_metadata = merge_dicts(\n",
      "                self.response_metadata, *(o.response_metadata for o in other)\n",
      "            )\n",
      "            return self.__class__(  # type: ignore[call-arg]\n",
      "                id=self.id,\n",
      "                content=content,\n",
      "                additional_kwargs=additional_kwargs,\n",
      "                response_metadata=response_metadata,\n",
      "            )\n",
      "---------------\n",
      "else:\n",
      "            raise TypeError(\n",
      "                'unsupported operand type(s) for +: \"'\n",
      "                f\"{self.__class__.__name__}\"\n",
      "                f'\" and \"{other.__class__.__name__}\"'\n",
      "            )\n",
      "---------------\n",
      "def message_to_dict(message: BaseMessage) -> dict:\n",
      "    \"\"\"Convert a Message to a dictionary.\n",
      "\n",
      "    Args:\n",
      "        message: Message to convert.\n",
      "\n",
      "    Returns:\n",
      "        Message as a dict. The dict will have a \"type\" key with the message type\n",
      "        and a \"data\" key with the message data as a dict.\n",
      "    \"\"\"\n",
      "    return {\"type\": message.type, \"data\": message.dict()}\n",
      "\n",
      "\n",
      "def messages_to_dict(messages: Sequence[BaseMessage]) -> List[dict]:\n",
      "    \"\"\"Convert a sequence of Messages to a list of dictionaries.\n",
      "\n",
      "    Args:\n",
      "        messages: Sequence of messages (as BaseMessages) to convert.\n",
      "\n",
      "    Returns:\n",
      "        List of messages as dicts.\n",
      "    \"\"\"\n",
      "    return [message_to_dict(m) for m in messages]\n",
      "\n",
      "\n",
      "def get_msg_title_repr(title: str, *, bold: bool = False) -> str:\n",
      "    \"\"\"Get a title representation for a message.\n",
      "\n",
      "    Args:\n",
      "        title: The title.\n",
      "        bold: Whether to bold the title. Default is False.\n",
      "\n",
      "    Returns:\n",
      "        The title representation.\n",
      "    \"\"\"\n",
      "    padded = \" \" + title + \" \"\n",
      "    sep_len = (80 - len(padded)) // 2\n",
      "    sep = \"=\" * sep_len\n",
      "    second_sep = sep + \"=\" if len(padded) % 2 else sep\n",
      "    if bold:\n",
      "        padded = get_bolded_text(padded)\n",
      "    return f\"{sep}{padded}{second_sep}\"\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "from app.agents.subgraphs.retrieval.chunking import chunk_with_AST_parser\n",
    "\n",
    "\n",
    "result = chunk_with_AST_parser(\n",
    "    \"/Users/minkijung/Documents/2PetProjects/AI_README_Generator/testing\"\n",
    ")\n",
    "for i, r in enumerate(result):\n",
    "    print(r.text)\n",
    "    print(\"---------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gitmeetup",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
