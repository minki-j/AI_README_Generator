{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPES_TO_INCLUDE = [\"class\", \"def\", \"identifier\", \"parameters\", \"argument_list\", \"->\", \":\", \"type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List\n",
    "\n",
    "\n",
    "def include_parent(\n",
    "    node: Any,\n",
    "    text: str,\n",
    "    include_parent_attributes: bool,\n",
    "    include_parent_init: bool,\n",
    "    depth: int,\n",
    "    buffer: str,\n",
    ") -> str:\n",
    "    \"\"\"Recursively include the parent of a node in the buffer. Depth 1 includes the same level nodes, depth 2 includes the same level and the parent's level, etc.\"\"\"\n",
    "    if node.type == \"block\":\n",
    "        parent = node.parent\n",
    "    else:\n",
    "        parent = node.parent.parent\n",
    "        if parent is None or parent.type == \"module\":\n",
    "            # pass for root or module\n",
    "            return buffer\n",
    "        if parent.type == \"block\":\n",
    "            # depending which level the node starts, a single level parent could be a block. We need a class of function definition which is one level up.\n",
    "            parent = parent.parent\n",
    "\n",
    "    parent_str = \"\"\n",
    "    byte_ranges = []\n",
    "    for child in parent.children:\n",
    "        if child.type in TYPES_TO_INCLUDE:\n",
    "            byte_ranges.append(child.byte_range)\n",
    "\n",
    "    # merge byte ranges\n",
    "    new_byte_ranges = []\n",
    "    merged_byte_ranges = ()\n",
    "    for i in range(len(byte_ranges) - 1):\n",
    "        if byte_ranges[i][1] == byte_ranges[i + 1][0] - 1:\n",
    "            if merged_byte_ranges == ():\n",
    "                merged_byte_ranges = (byte_ranges[i][0], byte_ranges[i + 1][1])\n",
    "            else:\n",
    "                merged_byte_ranges = (merged_byte_ranges[0], byte_ranges[i + 1][1])\n",
    "        else:\n",
    "            if merged_byte_ranges == ():\n",
    "                new_byte_ranges.append(byte_ranges[i])\n",
    "            else:\n",
    "                new_byte_ranges.append(merged_byte_ranges)\n",
    "                merged_byte_ranges = ()\n",
    "            if i == len(byte_ranges) - 1:\n",
    "                new_byte_ranges.append(byte_ranges[i + 1])\n",
    "\n",
    "    for byte_range in new_byte_ranges:\n",
    "        reach_the_begin = False\n",
    "        left_end_byte_of_chunk = byte_range[0]\n",
    "        while not reach_the_begin:\n",
    "            if text[left_end_byte_of_chunk - 1] != \" \":\n",
    "                reach_the_begin = True\n",
    "            else:\n",
    "                left_end_byte_of_chunk -= 1\n",
    "        parent_str += text[left_end_byte_of_chunk : byte_range[1] + 1]\n",
    "\n",
    "    # Add extra information about the parent node\n",
    "    if include_parent_attributes:\n",
    "        pass\n",
    "\n",
    "    if include_parent_init:\n",
    "        pass\n",
    "\n",
    "    if depth == 1:  # last level\n",
    "        return parent_str + \"\\n\" + buffer\n",
    "    else:\n",
    "        return include_parent(\n",
    "            parent,\n",
    "            text,\n",
    "            include_parent_attributes,\n",
    "            include_parent_init,\n",
    "            depth - 1,\n",
    "            parent_str + \"\\n\" + buffer,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_node(\n",
    "    node: Any,\n",
    "    text: str,\n",
    "    last_end: int = 0,\n",
    "    max_chars: int = 1500,\n",
    "    include_parent_depth: int = 0,\n",
    "    include_parent_attributes: bool = True,\n",
    "    # summarize_parent_attributes: bool = False,\n",
    "    include_parent_init: bool = True,\n",
    "    # summarize_parent_init: bool = False,\n",
    "    include_related_imports: bool = True,\n",
    ") -> List[str]:\n",
    "    new_chunks = []\n",
    "    imports = []\n",
    "    current_chunk = \"\"\n",
    "    is_first_node_for_chunk = True\n",
    "    for child in node.children:\n",
    "        # print(\n",
    "        #     \"----------------------\\n\\nprocessing node: \",\n",
    "        #     child.text[:30],\n",
    "        # )\n",
    "        # print(\"current_chunk:\\n---->>\", current_chunk, \"<<----\")\n",
    "        if child.end_byte - child.start_byte > max_chars:\n",
    "            # Child is too big, recursively chunk the child\n",
    "            # print(\"Child is too big, recursively chunk the child\")\n",
    "            if len(current_chunk) > 0:\n",
    "                new_chunks.append(current_chunk)\n",
    "            current_chunk = \"\"\n",
    "            new_chunks.extend(\n",
    "                chunk_node(\n",
    "                    child,\n",
    "                    text,\n",
    "                    last_end,\n",
    "                    max_chars,\n",
    "                    include_parent_depth,\n",
    "                    include_parent_attributes,\n",
    "                    include_parent_init,\n",
    "                    include_related_imports,\n",
    "                )\n",
    "            )\n",
    "            is_first_node_for_chunk = True\n",
    "        else:\n",
    "            # add related parts only for the first node of a chunk\n",
    "            if is_first_node_for_chunk and include_parent_depth > 0:\n",
    "                # print(\"include_parent of node: \", child.parent.text[:30])\n",
    "                parent_information = include_parent(\n",
    "                    child,\n",
    "                    text,\n",
    "                    include_parent_attributes,\n",
    "                    include_parent_init,\n",
    "                    include_parent_depth,\n",
    "                    \"\",\n",
    "                )\n",
    "                if len(parent_information) > 0:\n",
    "                    current_chunk += parent_information\n",
    "                current_chunk += text[last_end : child.end_byte]\n",
    "            else:\n",
    "                if len(current_chunk) + child.end_byte - last_end > max_chars:\n",
    "                    new_chunks.append(current_chunk)\n",
    "                    if include_parent_depth > 0:\n",
    "                        parent_information = include_parent(\n",
    "                            child,\n",
    "                            text,\n",
    "                            include_parent_attributes,\n",
    "                            include_parent_init,\n",
    "                            include_parent_depth,\n",
    "                            \"\",\n",
    "                        )\n",
    "                        current_chunk = (\n",
    "                            parent_information\n",
    "                            + text[last_end : child.end_byte]\n",
    "                        )\n",
    "                    else:\n",
    "                        current_chunk = text[last_end : child.end_byte]\n",
    "                else:\n",
    "                    current_chunk += text[last_end : child.end_byte]\n",
    "            is_first_node_for_chunk = False\n",
    "\n",
    "        last_end = child.end_byte\n",
    "\n",
    "    if len(current_chunk) > 0:\n",
    "        new_chunks.append(current_chunk)\n",
    "\n",
    "    return new_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/minkijung/anaconda3/envs/gitmeetup/lib/python3.12/site-packages/tree_sitter/__init__.py:36: FutureWarning: Language(path, name) is deprecated. Use Language(ptr, name) instead.\n",
      "  warn(\"{} is deprecated. Use {} instead.\".format(old, new), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import tree_sitter_languages\n",
    "parser = tree_sitter_languages.get_parser(\"python\")\n",
    "\n",
    "with open (\"test.py\", \"r\") as file:\n",
    "    text = file.read()\n",
    "tree = parser.parse(bytes(text, \"utf-8\"))\n",
    "root = tree.root_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = chunk_node(root, text, 0, 1500, 2, True, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"Code splitter.\"\"\"\n",
      "\n",
      "import os\n",
      "from typing import Any, Callable, List, Optional\n",
      "\n",
      "from llama_index.core.bridge.pydantic import Field, PrivateAttr\n",
      "from llama_index.core.callbacks.base import CallbackManager\n",
      "from llama_index.core.callbacks.schema import CBEventType, EventPayload\n",
      "from llama_index.core.node_parser.interface import TextSplitter\n",
      "from llama_index.core.node_parser.node_utils import default_id_func\n",
      "from llama_index.core.schema import Document\n",
      "\n",
      "DEFAULT_CHUNK_LINES = 40\n",
      "DEFAULT_LINES_OVERLAP = 15\n",
      "DEFAULT_MAX_CHARS = 1500\n",
      "---------------\n",
      "\n",
      "\n",
      "\n",
      "class CodeSplitter(TextSplitter):\n",
      "---------------\n",
      "class CodeSplitter((TextSplitter):\n",
      "\n",
      "    \"\"\"Split code using a AST parser.\n",
      "\n",
      "    Thank you to Kevin Lu / SweepAI for suggesting this elegant code splitting solution.\n",
      "    https://docs.sweep.dev/blogs/chunking-2m-files\n",
      "    \"\"\"\n",
      "\n",
      "    language: str = Field(\n",
      "        description=\"The programming language of the code being split.\"\n",
      "    )\n",
      "    chunk_lines: int = Field(\n",
      "        default=DEFAULT_CHUNK_LINES,\n",
      "        description=\"The number of lines to include in each chunk.\",\n",
      "        gt=0,\n",
      "    )\n",
      "    chunk_lines_overlap: int = Field(\n",
      "        default=DEFAULT_LINES_OVERLAP,\n",
      "        description=\"How many lines of code each chunk overlaps with.\",\n",
      "        gt=0,\n",
      "    )\n",
      "    max_chars: int = Field(\n",
      "        default=DEFAULT_MAX_CHARS,\n",
      "        description=\"Maximum number of characters per chunk.\",\n",
      "        gt=0,\n",
      "    )\n",
      "    _parser: Any = PrivateAttr()\n",
      "---------------\n",
      "class CodeSplitter((TextSplitter):\n",
      "\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        language: str,\n",
      "        chunk_lines: int = DEFAULT_CHUNK_LINES,\n",
      "        chunk_lines_overlap: int = DEFAULT_LINES_OVERLAP,\n",
      "        max_chars: int = DEFAULT_MAX_CHARS,\n",
      "        parser: Any = None,\n",
      "        callback_manager: Optional[CallbackManager] = None,\n",
      "        include_metadata: bool = True,\n",
      "        include_prev_next_rel: bool = True,\n",
      "        id_func: Optional[Callable[[int, Document], str]] = None,\n",
      "    ) -> None:\n",
      "---------------\n",
      "class CodeSplitter((TextSplitter):\n",
      "    def __init__((\n",
      "        self,\n",
      "        language: str,\n",
      "        chunk_lines: int = DEFAULT_CHUNK_LINES,\n",
      "        chunk_lines_overlap: int = DEFAULT_LINES_OVERLAP,\n",
      "        max_chars: int = DEFAULT_MAX_CHARS,\n",
      "        parser: Any = None,\n",
      "        callback_manager: Optional[CallbackManager] = None,\n",
      "        include_metadata: bool = True,\n",
      "        include_prev_next_rel: bool = True,\n",
      "        id_func: Optional[Callable[[int, Document], str]] = None,\n",
      "    ) -> None:\n",
      "\n",
      "        \"\"\"Initialize a CodeSplitter.\"\"\"\n",
      "        from tree_sitter import Parser  # pants: no-infer-dep\n",
      "\n",
      "        if parser is None:\n",
      "            try:\n",
      "                import tree_sitter_languages  # pants: no-infer-dep\n",
      "\n",
      "                print(\"language: \", language)\n",
      "                parser = tree_sitter_languages.get_parser(language)\n",
      "            except ImportError:\n",
      "                raise ImportError(\n",
      "                    \"Please install tree_sitter_languages to use CodeSplitter.\"\n",
      "                    \"Or pass in a parser object.\"\n",
      "                )\n",
      "            except Exception:\n",
      "                print(\n",
      "                    f\"Could not get parser for language {language}. Check \"\n",
      "                    \"https://github.com/grantjenks/py-tree-sitter-languages#license \"\n",
      "                    \"for a list of valid languages.\"\n",
      "                )\n",
      "                raise\n",
      "        if not isinstance(parser, Parser):\n",
      "            raise ValueError(\"Parser must be a tree-sitter Parser object.\")\n",
      "\n",
      "        self._parser = parser\n",
      "\n",
      "        callback_manager = callback_manager or CallbackManager([])\n",
      "        id_func = id_func or default_id_func\n",
      "\n",
      "        super().__init__(\n",
      "            language=language,\n",
      "            chunk_lines=chunk_lines,\n",
      "            chunk_lines_overlap=chunk_lines_overlap,\n",
      "            max_chars=max_chars,\n",
      "            callback_manager=callback_manager,\n",
      "            include_metadata=include_metadata,\n",
      "            include_prev_next_rel=include_prev_next_rel,\n",
      "            id_func=id_func,\n",
      "        )\n",
      "---------------\n",
      "class CodeSplitter((TextSplitter):\n",
      "\n",
      "\n",
      "    @classmethod\n",
      "    def from_defaults(\n",
      "        cls,\n",
      "        language: str,\n",
      "        chunk_lines: int = DEFAULT_CHUNK_LINES,\n",
      "        chunk_lines_overlap: int = DEFAULT_LINES_OVERLAP,\n",
      "        max_chars: int = DEFAULT_MAX_CHARS,\n",
      "        callback_manager: Optional[CallbackManager] = None,\n",
      "        parser: Any = None,\n",
      "    ) -> \"CodeSplitter\":\n",
      "        \"\"\"Create a CodeSplitter with default values.\"\"\"\n",
      "        return cls(\n",
      "            language=language,\n",
      "            chunk_lines=chunk_lines,\n",
      "            chunk_lines_overlap=chunk_lines_overlap,\n",
      "            max_chars=max_chars,\n",
      "            parser=parser,\n",
      "        )\n",
      "\n",
      "    @classmethod\n",
      "    def class_name(cls) -> str:\n",
      "        return \"CodeSplitter\"\n",
      "---------------\n",
      "class CodeSplitter((TextSplitter):\n",
      "\n",
      "\n",
      "    def _chunk_node(self, node: Any, text: str, last_end: int = 0) -> List[str]:\n",
      "        new_chunks = []\n",
      "        current_chunk = \"\"\n",
      "        for child in node.children:\n",
      "            if child.end_byte - child.start_byte > self.max_chars:\n",
      "                # Child is too big, recursively chunk the child\n",
      "                if len(current_chunk) > 0:\n",
      "                    new_chunks.append(current_chunk)\n",
      "                current_chunk = \"\"\n",
      "                new_chunks.extend(self._chunk_node(child, text, last_end))\n",
      "            elif (\n",
      "                len(current_chunk) + child.end_byte - child.start_byte > self.max_chars\n",
      "            ):\n",
      "                # Child would make the current chunk too big, so start a new chunk\n",
      "                new_chunks.append(current_chunk)\n",
      "                current_chunk = text[last_end : child.end_byte]\n",
      "            else:\n",
      "                current_chunk += text[last_end : child.end_byte]\n",
      "            last_end = child.end_byte\n",
      "        if len(current_chunk) > 0:\n",
      "            new_chunks.append(current_chunk)\n",
      "        return new_chunks\n",
      "---------------\n",
      "class CodeSplitter((TextSplitter):\n",
      "\n",
      "\n",
      "    def split_text(self, text: str) -> List[str]:\n",
      "        \"\"\"Split incoming code and return chunks using the AST.\"\"\"\n",
      "        with self.callback_manager.event(\n",
      "            CBEventType.CHUNKING, payload={EventPayload.CHUNKS: [text]}\n",
      "        ) as event:\n",
      "            tree = self._parser.parse(bytes(text, \"utf-8\"))\n",
      "\n",
      "            if (\n",
      "                not tree.root_node.children\n",
      "                or tree.root_node.children[0].type != \"ERROR\"\n",
      "            ):\n",
      "                chunks = [\n",
      "                    chunk.strip() for chunk in self._chunk_node(tree.root_node, text)\n",
      "                ]\n",
      "                event.on_end(\n",
      "                    payload={EventPayload.CHUNKS: chunks},\n",
      "                )\n",
      "\n",
      "                return chunks\n",
      "            else:\n",
      "                raise ValueError(f\"Could not parse code with language {self.language}.\")\n",
      "\n",
      "        # TODO: set up auto-language detection using something like https://github.com/yoeo/guesslang.\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "for i,result in enumerate(chunks):\n",
    "    print(result)\n",
    "    print(\"---------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gitmeetup",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
